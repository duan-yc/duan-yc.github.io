---
layout: post
title: "DDRNet"
subtitle: "semantic segmentation"
author: "DYC"
header-img: "img/ss.jpg"
header-mask: 0.3
catalog: true
tags:
  - semantic segmentation

---

#### DDRNet

##### **motivation**

- 现在语义分割模型很多通过消耗时间来提高性能，很多实时的网络

##### contribution

- 提出了一组用于实时语义分割的高效骨干网，提出的深度双分辨率网络由两个深度分支组成，在两个分支之间进行多次双边融合
- 设计了一种新的上下文信息提取器DAPPM模块，能够扩大感受野和融合多尺度低分辨率的上下文内容
- DDRNets在分割精度和推理速度之间取得了很好的平衡，并且在Cityscapes and CamVid dataset达到了最先进的权衡

##### architecture

在分类主干上添加一个额外的高分辨率分支，高分辨率分支不包含任何下采样操作，并且与低分辨率分支具有一对一的对应关系，以形成深度高分辨率表示。然后在不同阶段进行多次双侧特征融合，充分融合空间信息和语义信息。

![image-20230904150657973](https://cdn.jsdelivr.net/gh/ddyycc123/imageloader@main/image-20230904150657973.png)

双侧特征融合包括将高分辨率分支融合到低分辨率分支（高到低）和将低分辨率分支融合到高分辨率分支（低到高）。对于高到低的融合，高分辨率特征图在点向求和之前通过3×3卷积序列进行下采样，其步幅为2，对于低到高的融合，低分辨率特征图首先通过1×1卷积进行压缩，然后使用双线性插值进行上采样

![image-20230904154439378](https://cdn.jsdelivr.net/gh/ddyycc123/imageloader@main/image-20230904154439378.png)

DDRNet结构细节如下表

![image-20230904154654438](https://cdn.jsdelivr.net/gh/ddyycc123/imageloader@main/image-20230904154654438.png)

**DAPPM**

下图是DAPPM的结构，以1/64图像分辨率的特征图为输入，再进行下采样生成1/128，1/256，1/512的特征映射，然后对各个层进行上采样，然后利用3*3的conv进行不同尺度的上下文信息的融合，更大池化核提取的上下文与更深的信息流相融合，不同深度与不同大小池化核的融合形成了多尺度性质，并且由于输入分辨率就仅为原图像的1/64，所以推理速度很快

![image-20230904154803525](https://cdn.jsdelivr.net/gh/ddyycc123/imageloader@main/image-20230904154803525.png)

**损失函数**

参照PSPNet将α设置为0.4，Lf、Ln、La分别为最终损耗、正态损耗、辅助损耗

![image-20230904162640704](https://cdn.jsdelivr.net/gh/ddyycc123/imageloader@main/image-20230904162640704.png)



```python
# Copyright (c) OpenMMLab. All rights reserved.
import torch.nn as nn
from mmcv.cnn import ConvModule, build_norm_layer
from mmengine.model import BaseModule

from mmseg.models.utils import DAPPM, BasicBlock, Bottleneck, resize
from mmseg.registry import MODELS
from mmseg.utils import OptConfigType


@MODELS.register_module()
class DDRNet(BaseModule):
    """DDRNet backbone.

    This backbone is the implementation of `Deep Dual-resolution Networks for
    Real-time and Accurate Semantic Segmentation of Road Scenes
    <http://arxiv.org/abs/2101.06085>`_.
    Modified from https://github.com/ydhongHIT/DDRNet.

    Args:
        in_channels (int): Number of input image channels. Default: 3.
        channels: (int): The base channels of DDRNet. Default: 32.
        ppm_channels (int): The channels of PPM module. Default: 128.
        align_corners (bool): align_corners argument of F.interpolate.
            Default: False.
        norm_cfg (dict): Config dict to build norm layer.
            Default: dict(type='BN', requires_grad=True).
        act_cfg (dict): Config dict for activation layer.
            Default: dict(type='ReLU', inplace=True).
        init_cfg (dict, optional): Initialization config dict.
            Default: None.
    """

    def __init__(self,
                 in_channels: int = 3,
                 channels: int = 32,
                 ppm_channels: int = 128,
                 align_corners: bool = False,
                 norm_cfg: OptConfigType = dict(type='BN', requires_grad=True),
                 act_cfg: OptConfigType = dict(type='ReLU', inplace=True),
                 init_cfg: OptConfigType = None):
        super().__init__(init_cfg)

        self.in_channels = in_channels
        self.ppm_channels = ppm_channels

        self.norm_cfg = norm_cfg
        self.act_cfg = act_cfg
        self.align_corners = align_corners

        # stage 0-2
        self.stem = self._make_stem_layer(in_channels, channels, num_blocks=2)
        self.relu = nn.ReLU()

        # low resolution(context) branch
        self.context_branch_layers = nn.ModuleList()
        for i in range(3):
            self.context_branch_layers.append(
                self._make_layer(
                    block=BasicBlock if i < 2 else Bottleneck,
                    inplanes=channels * 2**(i + 1),
                    planes=channels * 8 if i > 0 else channels * 4,
                    num_blocks=2 if i < 2 else 1,
                    stride=2))

        # bilateral fusion
        self.compression_1 = ConvModule(
            channels * 4,
            channels * 2,
            kernel_size=1,
            norm_cfg=self.norm_cfg,
            act_cfg=None)
        self.down_1 = ConvModule(
            channels * 2,
            channels * 4,
            kernel_size=3,
            stride=2,
            padding=1,
            norm_cfg=self.norm_cfg,
            act_cfg=None)

        self.compression_2 = ConvModule(
            channels * 8,
            channels * 2,
            kernel_size=1,
            norm_cfg=self.norm_cfg,
            act_cfg=None)
        self.down_2 = nn.Sequential(
            ConvModule(
                channels * 2,
                channels * 4,
                kernel_size=3,
                stride=2,
                padding=1,
                norm_cfg=self.norm_cfg,
                act_cfg=self.act_cfg),
            ConvModule(
                channels * 4,
                channels * 8,
                kernel_size=3,
                stride=2,
                padding=1,
                norm_cfg=self.norm_cfg,
                act_cfg=None))

        # high resolution(spatial) branch
        self.spatial_branch_layers = nn.ModuleList()
        for i in range(3):
            self.spatial_branch_layers.append(
                self._make_layer(
                    block=BasicBlock if i < 2 else Bottleneck,
                    inplanes=channels * 2,
                    planes=channels * 2,
                    num_blocks=2 if i < 2 else 1,
                ))

        self.spp = DAPPM(
            channels * 16, ppm_channels, channels * 4, num_scales=5)

    def _make_stem_layer(self, in_channels, channels, num_blocks):
        layers = [
            ConvModule(
                in_channels,
                channels,
                kernel_size=3,
                stride=2,
                padding=1,
                norm_cfg=self.norm_cfg,
                act_cfg=self.act_cfg),
            ConvModule(
                channels,
                channels,
                kernel_size=3,
                stride=2,
                padding=1,
                norm_cfg=self.norm_cfg,
                act_cfg=self.act_cfg)
        ]

        layers.extend([
            self._make_layer(BasicBlock, channels, channels, num_blocks),
            nn.ReLU(),
            self._make_layer(
                BasicBlock, channels, channels * 2, num_blocks, stride=2),
            nn.ReLU(),
        ])

        return nn.Sequential(*layers)

    def _make_layer(self, block, inplanes, planes, num_blocks, stride=1):
        downsample = None
        if stride != 1 or inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(
                    inplanes,
                    planes * block.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False),
                build_norm_layer(self.norm_cfg, planes * block.expansion)[1])

        layers = [
            block(
                in_channels=inplanes,
                channels=planes,
                stride=stride,
                downsample=downsample)
        ]
        inplanes = planes * block.expansion
        for i in range(1, num_blocks):
            layers.append(
                block(
                    in_channels=inplanes,
                    channels=planes,
                    stride=1,
                    norm_cfg=self.norm_cfg,
                    act_cfg_out=None if i == num_blocks - 1 else self.act_cfg))

        return nn.Sequential(*layers)

    def forward(self, x):
        """Forward function."""
        #input size [6,3,1024,1024]
        out_size = (x.shape[-2] // 8, x.shape[-1] // 8)

        # stage 0-2
        # stem(x)的维度[6, 64, 128, 128]
        x = self.stem(x)

        # stage3
        # x_c的维度[6, 128, 64, 64],x_s的维度[6, 64, 128, 128]
        x_c = self.context_branch_layers[0](x)
        x_s = self.spatial_branch_layers[0](x)
        comp_c = self.compression_1(self.relu(x_c))
        x_c += self.down_1(self.relu(x_s))
        x_s += resize(
            comp_c,
            size=out_size,
            mode='bilinear',
            align_corners=self.align_corners)
        if self.training:
            temp_context = x_s.clone()
        # stage4
        # x_c的维度[6, 128, 64, 64],x_s的维度[6, 64, 128, 128]
        x_c = self.context_branch_layers[1](self.relu(x_c))
        x_s = self.spatial_branch_layers[1](self.relu(x_s))
        comp_c = self.compression_2(self.relu(x_c))
        x_c += self.down_2(self.relu(x_s))
        x_s += resize(
            comp_c,
            size=out_size,
            mode='bilinear',
            align_corners=self.align_corners)
        # stage5
        # x_c的维度[6, 256, 32, 32],x_s的维度[6, 64, 128, 128]      
        x_s = self.spatial_branch_layers[2](self.relu(x_s))
        x_c = self.context_branch_layers[2](self.relu(x_c))
        x_c = self.spp(x_c)
        x_c = resize(
            x_c,
            size=out_size,
            mode='bilinear',
            align_corners=self.align_corners)
        # x_c的维度[6, 128, 128, 128],x_s的维度[6, 128, 128, 128],temp_context的维度[6, 64, 128, 128]   
        return (temp_context, x_s + x_c) if self.training else x_s + x_c

```

